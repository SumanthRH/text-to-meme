{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b68b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math, statistics, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# HF token\n",
    "token = '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'\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "api=HfApi()\n",
    "folder=HfFolder()\n",
    "api.set_access_token(token)\n",
    "folder.save_token(token)\n",
    "\n",
    "# constants\n",
    "dataset = \"dank_memes\"\n",
    "pre_trained_model_checkpoint = \"roberta-base\"\n",
    "model_name = \"roberta-base-memes-900k-subset-75\"\n",
    "hub_model_id = \"armageddon/roberta-base-memes-900k-subset-75\"\n",
    "stride = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302e5b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in meme dict dataset: dict_keys(['label_uuid_dic', 'uuid_label_dic', 'uuid_caption_dic', 'uuid_image_path_dic'])\n",
      "Number of uuids: 300\n"
     ]
    }
   ],
   "source": [
    "# load meme dataset\n",
    "meme_dict = None\n",
    "with open('./meme_900k_cleaned_data.pkl', 'rb') as f:\n",
    "    meme_dict = pickle.load(f)\n",
    "print(\"Keys in meme dict dataset:\", meme_dict.keys())\n",
    "print(\"Number of uuids:\", len(meme_dict['uuid_label_dic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682e8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def clean_and_unify_caption(caption):\n",
    "    return caption[0].strip()+', '+caption[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9368f4f4-8cae-44cf-9b88-cbef5bd4c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select the uuids\n",
    "# import os\n",
    "# import regex as re\n",
    "# training_uuids = []\n",
    "# dir_path = './memes900k_qa/'\n",
    "# for path in os.listdir(dir_path):\n",
    "#     if os.path.isfile(os.path.join(dir_path, path)):\n",
    "#         if not re.match(r'.*_manual.pkl', path):\n",
    "#             training_uuids.append(path.split('.')[0])\n",
    "# print(len(training_uuids))\n",
    "# labels = {k:v for k,v in zip(training_uuids, range(len(training_uuids)))}\n",
    "\n",
    "# with open('./models/data/training_label.pkl', 'wb') as f:\n",
    "#     pickle.dump(labels, f)\n",
    "\n",
    "with open('./models/data/training_label.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ebb3bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000 22500 22500\n"
     ]
    }
   ],
   "source": [
    "# # create pandas dataframe\n",
    "# temp_arr = []\n",
    "# for uuid in training_uuids:\n",
    "#     for caption in meme_dict['uuid_caption_dic'][uuid]:\n",
    "#         temp_arr.append([uuid, clean_and_unify_caption(caption)])\n",
    "# df = pd.DataFrame(temp_arr, columns=['category', 'text'])\n",
    "\n",
    "# # split dataset\n",
    "# np.random.seed(42)\n",
    "# df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "#                                      [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "# print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3428e9f-b93c-4a2b-accb-f410aa61866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length = 50, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6edb8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset(df_train)\n",
    "# val_dataset = Dataset(df_val)\n",
    "# test_dataset = Dataset(df_test)\n",
    "train_dataset = torch.load('./models/data/train_dataset')\n",
    "val_dataset = torch.load('./models/data/val_dataset')\n",
    "test_dataset = torch.load('./models/data/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74eacce2-f2ba-4bbe-bc49-7eb98f6a6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoModel\n",
    "class Meme_Classifier(nn.Module):\n",
    "    def __init__(self, num_labels, dropout=0.3):\n",
    "        super(Meme_Classifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(pre_trained_model_checkpoint)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 512)\n",
    "        self.linear2 = nn.Linear(512, num_labels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.model(input_ids=input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output1 = self.dropout(pooled_output)\n",
    "        linear_output1 = self.dropout(self.relu(self.linear1(dropout_output1)))\n",
    "        final_output = self.relu(self.linear2(linear_output1))\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c9fdb1a-17bc-4ed9-a536-2936be61ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, train_dataset, val_dataset, learning_rate, loss_diff, max_epochs):\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    epoch_num = 0\n",
    "    prev_loss = float('inf')\n",
    "    while True:\n",
    "            epoch_num+=1\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataset): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_dataset): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_dataset): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_dataset): .3f}')\n",
    "            \n",
    "            if epoch_num>=max_epochs or abs(prev_loss-total_loss_train)<=loss_diff:\n",
    "                break\n",
    "            prev_loss=total_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9087c56-242d-42e0-9046-d304d0514a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 5625/5625 [06:54<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.120                 | Train Accuracy:  0.270                 | Val Loss:  0.107                 | Val Accuracy:  0.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.097                 | Train Accuracy:  0.389                 | Val Loss:  0.090                 | Val Accuracy:  0.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:55<00:00, 13.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.084                 | Train Accuracy:  0.425                 | Val Loss:  0.081                 | Val Accuracy:  0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.076                 | Train Accuracy:  0.452                 | Val Loss:  0.075                 | Val Accuracy:  0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.071                 | Train Accuracy:  0.476                 | Val Loss:  0.072                 | Val Accuracy:  0.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:53<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.067                 | Train Accuracy:  0.498                 | Val Loss:  0.069                 | Val Accuracy:  0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:53<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.064                 | Train Accuracy:  0.518                 | Val Loss:  0.067                 | Val Accuracy:  0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.061                 | Train Accuracy:  0.535                 | Val Loss:  0.066                 | Val Accuracy:  0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.059                 | Train Accuracy:  0.550                 | Val Loss:  0.065                 | Val Accuracy:  0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.057                 | Train Accuracy:  0.564                 | Val Loss:  0.065                 | Val Accuracy:  0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.055                 | Train Accuracy:  0.577                 | Val Loss:  0.064                 | Val Accuracy:  0.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.052                 | Train Accuracy:  0.594                 | Val Loss:  0.064                 | Val Accuracy:  0.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.050                 | Train Accuracy:  0.608                 | Val Loss:  0.064                 | Val Accuracy:  0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.048                 | Train Accuracy:  0.622                 | Val Loss:  0.064                 | Val Accuracy:  0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | Train Loss:  0.045                 | Train Accuracy:  0.639                 | Val Loss:  0.064                 | Val Accuracy:  0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | Train Loss:  0.043                 | Train Accuracy:  0.656                 | Val Loss:  0.065                 | Val Accuracy:  0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | Train Loss:  0.040                 | Train Accuracy:  0.674                 | Val Loss:  0.066                 | Val Accuracy:  0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:45<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | Train Loss:  0.038                 | Train Accuracy:  0.692                 | Val Loss:  0.066                 | Val Accuracy:  0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:53<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | Train Loss:  0.035                 | Train Accuracy:  0.711                 | Val Loss:  0.067                 | Val Accuracy:  0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [06:54<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 21 | Train Loss:  0.033                 | Train Accuracy:  0.732                 | Val Loss:  0.069                 | Val Accuracy:  0.514\n"
     ]
    }
   ],
   "source": [
    "model = Meme_Classifier(len(labels))\n",
    "LR = 1e-6\n",
    "max_epochs = 20\n",
    "loss_diff = 0.01\n",
    "train(model, train_dataset, val_dataset, LR, loss_diff, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2208190c-0c7d-4527-b108-9ec5b00c5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "MODEL_PATH = './models/roberta-base-memes-900k-subset-75'\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67543cc8-cd3d-4f25-a180-a12cf00802ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Meme_Classifier(len(labels))\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8db6ea82-2e85-4866-9990-3fded4f98f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:16<00:00, 42.47it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "acc = 0\n",
    "for test_input, test_label in tqdm(test_dataloader):\n",
    "    mask = test_input['attention_mask'].to(device)\n",
    "    input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "    logits = model(input_id, mask).to('cpu')\n",
    "    acc += meme_accuracy_sum_only(logits, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2a138b5-8685-463b-856e-1cd0236a48ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6573333333333333\n"
     ]
    }
   ],
   "source": [
    "test_accuracy  = acc/len(test_dataset.labels)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7a3ad-9333-403d-b7de-79ca9b64f1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b86db23-ec90-4fcd-be34-ede25d54b3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:00<00:00, 10827.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# now test full user captions for accuracy\n",
    "import os\n",
    "import regex as re\n",
    "import pickle\n",
    "testing_user_captions = []\n",
    "dir_path = './memes900k_qa/'\n",
    "for path in tqdm(os.listdir(dir_path)):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        if not re.match(r'.*_manual.pkl', path):\n",
    "            with open(os.path.join(dir_path, path), 'rb') as f:\n",
    "                dic = pickle.load(f)\n",
    "                for v in dic['qa'].keys(): \n",
    "                    testing_user_captions.append([v, labels[dic['uuid']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3842c0a-b11b-47a9-a3e4-a82f6be19702",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tokenizer(text[0], padding='max_length', max_length = 50, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in testing_user_captions]\n",
    "# input_ids = torch.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "208b46eb-d57d-4890-a873-55ffbb2d3df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  7424,   939,  3529,    65,    55, 15711,     9, 19982,  1666,\n",
       "             6,   117, 16506,    47, 14964,   101,   195,   416,   328,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19ac681f-d6e5-4fcb-87bb-e58371394884",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.stack([x['input_ids'] for x in tokenized])\n",
    "input_ids = input_ids.reshape(len(input_ids), -1).to(device)\n",
    "masks = torch.stack([x['attention_mask'] for x in tokenized])\n",
    "masks = masks.reshape(len(input_ids), -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fffe7da1-40e5-4aad-83d2-f74ef1bd4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3745/3745 [01:25<00:00, 43.71it/s]\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in tqdm(range(len(input_ids))):\n",
    "    logits = model(input_ids[i].reshape(1, -1), masks[i].reshape(1, -1))\n",
    "    acc += meme_accuracy_sum_only(logits, [testing_user_captions[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "875a969f-c668-482e-8799-b194d737e009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6085447263017356"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc/len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8f1d0-a9f3-401e-a88e-3d0c5b9c7d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yash",
   "language": "python",
   "name": "yash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
