{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981d20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29855627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2a27e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\", \"This is a demonstration\"]\n",
    "\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = sentence_model.encode(sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0df2c933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40455922"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embeddings[0], embeddings[1])/ (np.linalg.norm(embeddings[0])* np.linalg.norm(embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9951c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.475783"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embeddings[0], embeddings[2])/ (np.linalg.norm(embeddings[0])* np.linalg.norm(embeddings[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "887db7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(embed1, embed2):\n",
    "    assert embed1.shape == embed2.shape, \\\n",
    "    f\"Both embeddings must be of same shape, got {embed1.shape} and {embed2.shape}\"\n",
    "    return np.dot(embed1, embed2)/ (np.linalg.norm(embed1)*np.linalg.norm(embed2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8bca211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import os.path\n",
    "from os import path\n",
    "from IPython.display import Image, display\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1936baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meme_900k_cleaned_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02bec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 573.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for uuid in tqdm(data['uuid_caption_dic']):\n",
    "    captions = data['uuid_caption_dic'][uuid]\n",
    "    captions = [' '.join(caption) for caption in captions]\n",
    "    captions = [caption.replace('<emp>', '') for caption in captions]\n",
    "    data['uuid_caption_dic'][uuid] = captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efacf3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:48<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# meme_embeddings = {}\n",
    "\n",
    "# for uuid in tqdm(data['uuid_caption_dic']):\n",
    "#     captions = data['uuid_caption_dic'][uuid]\n",
    "#     embeddings = model.encode(captions)\n",
    "#     average = np.mean(embeddings,  axis=0)\n",
    "#     meme_embeddings[uuid] = average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382beef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"meme_embeddings.pkl\", 'wb') as f:\n",
    "#     pickle.dump(meme_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8055b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"meme_embeddings.pkl\", 'rb') as f:\n",
    "    meme_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cf28f169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 jets y u no get good quarterbacks?!\n"
     ]
    }
   ],
   "source": [
    "all_uuids = list(data['uuid_caption_dic'].keys())\n",
    "\n",
    "niter = 50\n",
    "random_similarities = []\n",
    "true_similarities = []\n",
    "candidate_cap_index = [np.random.choice(len(data['uuid_caption_dic'][uuid])) for uuid in all_uuids]\n",
    "candidate_captions = [data['uuid_caption_dic'][all_uuids[i]][ind] for i, ind in enumerate(candidate_cap_index)]\n",
    "print(len(candidate_captions), candidate_captions[0])\n",
    "for embed_uuid, caption in zip(all_uuids, candidate_captions):\n",
    "    embedding = sentence_model.encode([caption])[0]\n",
    "    average_sim = 0\n",
    "    for i in range(niter):\n",
    "        random_uuid = embed_uuid\n",
    "        while random_uuid == embed_uuid:\n",
    "            random_uuid = np.random.choice(all_uuids)\n",
    "        random_meme_embedding  = meme_embeddings[random_uuid]\n",
    "        average_sim += get_cosine_sim(embedding, random_meme_embedding)\n",
    "    average_sim /= niter\n",
    "    true_similarity = get_cosine_sim(embedding, meme_embeddings[embed_uuid])\n",
    "    true_similarities.append(true_similarity)\n",
    "    random_similarities.append(average_sim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1183c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2462124640348251"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(random_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0a8a9bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37961653"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(true_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83bf3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:00<00:00, 2611.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# similarity score \n",
    "\n",
    "# now test full user captions for accuracy\n",
    "import os\n",
    "import regex as re\n",
    "import pickle\n",
    "testing_user_captions = []\n",
    "dir_path = './memes900k_qa/'\n",
    "for path in tqdm(os.listdir(dir_path)):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        if not re.match(r'.*_manual.pkl', path):\n",
    "            with open(os.path.join(dir_path, path), 'rb') as f:\n",
    "                dic = pickle.load(f)\n",
    "                for v in dic['qa'].keys(): \n",
    "                    testing_user_captions.append([v, dic['uuid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86e37205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5c6a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_user_prompts_and_uuids = random.choices(testing_user_captions, k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e846621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_user_prompts = [candidate_tuple[0] for candidate_tuple in candidate_user_prompts_and_uuids]\n",
    "candidate_uuids = [candidate_tuple[1] for candidate_tuple in candidate_user_prompts_and_uuids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b3e3968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [32:07<00:00,  6.43s/it]\n"
     ]
    }
   ],
   "source": [
    "new_meme_embeddings = {}\n",
    "for uuid in tqdm(data['uuid_caption_dic']):\n",
    "    captions = data['uuid_caption_dic'][uuid][:1000]\n",
    "    embeddings = model.encode(captions)\n",
    "    average = np.mean(embeddings,  axis=0)\n",
    "    new_meme_embeddings[uuid] = average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8fdffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"user_prompt_meme_embeddings.pkl\", 'wb') as f:\n",
    "#     pickle.dump(new_meme_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf39f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_captions = []\n",
    "for uuid in candidate_uuids:\n",
    "    caption = np.random.choice(data['uuid_caption_dic'][uuid][1000:3000])\n",
    "    candidate_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d6c6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sims = []\n",
    "caption_sims = []\n",
    "\n",
    "for i, uuid in enumerate(candidate_uuids):\n",
    "    meme_embedding = new_meme_embeddings[uuid]\n",
    "    caption_embedding = model.encode([candidate_captions[i]])[0]\n",
    "    user_embedding = model.encode([candidate_user_prompts[i]])[0]\n",
    "    caption_sims.append(get_cosine_sim(caption_embedding, meme_embedding))\n",
    "    user_sims.append(get_cosine_sim(user_embedding, meme_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "179e7dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38320404"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(caption_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37444e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3654713"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(user_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5903ac",
   "metadata": {},
   "source": [
    "# CLIP Baseline\n",
    "\n",
    "For every meme caption, we try to find the best matching image through CLIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc4a3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0e527145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 72.33it/s]\n"
     ]
    }
   ],
   "source": [
    "uuid_feature_dic = {}\n",
    "all_paths = list(data['uuid_image_path_dic'].values())\n",
    "all_uuids = list(data['uuid_caption_dic'].keys())\n",
    "for i in tqdm(range(len(all_paths))):\n",
    "    uuid = all_uuids[i]\n",
    "    img_path = data['uuid_image_path_dic'][uuid]\n",
    "    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    uuid_feature_dic[uuid] = image_features.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1bf84e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./memes900k/images/bane-permission-to-die.jpg'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fbf879fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid_feature_dic[uuid].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "47f37903",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "all_uuid_inds = []\n",
    "for i, uuid in enumerate(data['uuid_caption_dic'].keys()):\n",
    "    captions = data['uuid_caption_dic'][uuid]\n",
    "    all_captions.extend(captions)\n",
    "    all_uuid_inds.extend([i for _ in captions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "487a6712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['uuid_caption_dic'].keys()) == list(uuid_feature_dic.keys()) # Need to match for getting top k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f5d96d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ea45bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_caps = []\n",
    "num_batches = len(all_captions)// batch_size\n",
    "for i in range(num_batches):\n",
    "    batch = all_captions[i*batch_size: (i+1)*batch_size]\n",
    "    batch_tokenized = clip.tokenize(batch).to(device)\n",
    "    tokenized_caps.append(batch_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03bc532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   953,   649,  ...,     0,     0,     0],\n",
       "        [49406,   340,   692,  ...,     0,     0,     0],\n",
       "        [49406,  2543,  1563,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [49406,   827,  2943,  ...,     0,     0,     0],\n",
       "        [49406,   827,   592,  ...,     0,     0,     0],\n",
       "        [49406,   592,   720,  ...,     0,     0,     0]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9a1f28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [16:29<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_feats = []\n",
    "with torch.no_grad():\n",
    "    for batch_tokenized in tqdm(tokenized_caps):\n",
    "        batch_text_feats = model.encode_text(batch_tokenized)\n",
    "        batch_text_feats /= batch_text_feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats.append(batch_text_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6c964e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = torch.vstack(list(uuid_feature_dic.values())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad4206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6e08d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features /= image_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2f269d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats = []\n",
    "for i,batch_text_feats in enumerate(text_feats):\n",
    "    new_batch = []\n",
    "    for j, text_feat in enumerate(batch_text_feats):\n",
    "        ind = i*batch_size + j\n",
    "        uuid_ind = all_uuid_inds[ind]\n",
    "        image_feat = image_features[uuid_ind]\n",
    "        new_batch.append(image_feat)\n",
    "    new_batch = torch.vstack(new_batch).to(device)\n",
    "    new_feats.append(new_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e327dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inds = []\n",
    "for i,batch_text_feats in enumerate(text_feats):\n",
    "    for j, text_feat in enumerate(batch_text_feats):\n",
    "        ind = i*batch_size + j\n",
    "        all_inds.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a145cb4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Error at 50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_100/3499325516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error at {i}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Error at 50"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(new_feats):\n",
    "    assert (batch.norm(dim=-1) == 1).all(), f\"Error at {i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "977a159b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 0.9995, 1.0000, 1.0000,\n",
       "        1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000,\n",
       "        1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 0.9995, 1.0000, 0.9995, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.9995, 0.9995, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.9995, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.9995, 1.0000, 0.9995, 0.9995, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995,\n",
       "        0.9995, 1.0000, 0.9995, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000,\n",
       "        0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995,\n",
       "        0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        0.9995, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 0.9995, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9995, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 0.9995, 0.9995,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 0.9995,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995,\n",
       "        1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 0.9995, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ce4cb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = []\n",
    "all_indices = []\n",
    "for batch_text_feats in text_feats:\n",
    "    similarity = (100.0 * batch_text_feats @ image_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(100)\n",
    "    values = values.cpu().numpy().tolist()\n",
    "    all_values.extend(values)\n",
    "    indices = indices.cpu().numpy().tolist()\n",
    "    all_indices.extend(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9bb00185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(top_indices, true_indices, k, debug=False):\n",
    "    assert len(top_indices) and len(true_indices), \\\n",
    "    f\"Inputs should have non zero length, got {len(top_indices)} and {len(true_indices)}\"\n",
    "    assert len(top_indices[0]) >= k, f\"Length should be atleast {k}, got {len(top_indices[0])}\"\n",
    "    accuracy = 0\n",
    "    if debug:\n",
    "        debug_set = set()\n",
    "    for i in range(len(top_indices)):\n",
    "        if true_indices[i] in top_indices[i][:k]:\n",
    "            accuracy += 1\n",
    "        elif debug and true_indices[i] not in debug_set:\n",
    "            debug_set.add(true_indices[i])\n",
    "    if debug:\n",
    "        print(\"No matches at:\\n\")\n",
    "        print(debug_set)\n",
    "    accuracy /= len(top_indices)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cbf927d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 900000)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_uuid_inds), len(all_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3a5d3d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26787666666666665"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k(all_indices, all_uuid_inds, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1c4aa4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2111888888888889"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k(all_indices, all_uuid_inds, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0a877962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09203"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k(all_indices, all_uuid_inds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "84faea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check with image features\n",
    "all_sanity_values = []\n",
    "all_sanity_indices = []\n",
    "# assert (image_features.norm(dim=-1) == 1 ).all()\n",
    "for i, batch_text_feats in enumerate(new_feats):\n",
    "#     assert (batch_text_feats.norm(dim=-1) == 1).all(), f\"Error at {i}\"\n",
    "    similarity = (100.0 * batch_text_feats @ image_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(100)\n",
    "    values = values.cpu().numpy().tolist()\n",
    "    all_sanity_values.extend(values)\n",
    "    indices = indices.cpu().numpy().tolist()\n",
    "    all_sanity_indices.extend(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e7d50",
   "metadata": {},
   "source": [
    "# Template Relevance Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b0305241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:02<00:00, 139.52it/s]\n"
     ]
    }
   ],
   "source": [
    "clip_similarities= []\n",
    "for i in tqdm(range(len(candidate_captions))):\n",
    "    embedding = sentence_model.encode([candidate_captions[i]])[0]\n",
    "    ind = i*3000 + candidate_cap_index[i]\n",
    "    retrieved_meme_uuid = all_uuids[all_indices[ind][0]] # take the first retrieved image\n",
    "    retrieved_meme_embedding = meme_embeddings[retrieved_meme_uuid]\n",
    "    sim = get_cosine_sim(embedding, retrieved_meme_embedding)\n",
    "    clip_similarities.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "000a6751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30860755"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clip_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdaf5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
